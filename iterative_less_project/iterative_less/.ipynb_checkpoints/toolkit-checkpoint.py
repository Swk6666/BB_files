# /root/iterative_less_project/iterative_less/toolkit.py
import os
import subprocess
import torch
import logging
import shutil
import json
import pandas as pd
import time
import torch.nn.functional as F
from . import config
from transformers import AutoModelForCausalLM
from peft import PeftModel

logger = logging.getLogger(__name__)

# --- ã€æ ¸å¿ƒä¿®å¤ã€‘ ---
# é‡å†™å‘½ä»¤æ‰§è¡Œå‡½æ•°ï¼Œé‡‡ç”¨ process.communicate() æ¥å½»åº•é¿å…I/Oæ­»é”ã€‚
def _run_command_final(command: str, description: str, is_gpu_intensive: bool = True, use_progress_log: bool = False):
    """
    æœ€ç»ˆä¿®å¤ç‰ˆçš„å‘½ä»¤æ‰§è¡Œå™¨ã€‚
    å®ƒé€šè¿‡ç­‰å¾…å­è¿›ç¨‹å®Œæˆåå†ç”¨communicate()è¯»å–è¾“å‡ºæ¥é¿å…I/Oæ­»é”ï¼Œ
    åŒæ—¶ä¿ç•™äº†é€šè¿‡progress.jsonæ–‡ä»¶è¿›è¡ŒçŠ¶æ€ç›‘æ§çš„èƒ½åŠ›ã€‚
    """
    if is_gpu_intensive:
        command = f"PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True {command}"

    progress_log_file = None
    if use_progress_log:
        # åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„è¿›åº¦æ—¥å¿—æ–‡ä»¶å
        progress_log_file = os.path.join(config.ITERATIVE_OUTPUT_DIR, f"progress_{int(time.time())}_{os.getpid()}.json")
        command = f"{command} --progress_log_file {progress_log_file}"
    
    logger.info(f"å³å°†æ‰§è¡Œ [{description}]...")
    logger.debug(f"å®Œæ•´å‘½ä»¤:\n---\n{command.strip()}\n---")
    
    process = subprocess.Popen(
        command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
        text=True, encoding='utf-8', cwd=config.PROJECT_ROOT
    )

    try:
        # ç›‘æ§é˜¶æ®µï¼šåªé€šè¿‡æ–‡ä»¶ç³»ç»Ÿç›‘æ§è¿›åº¦ï¼Œä¸æ¥è§¦I/Oç®¡é“
        if use_progress_log and progress_log_file:
            last_message = ""
            print() # æ¢è¡Œä»¥å¼€å§‹æ–°çš„è¿›åº¦æ¡
            while process.poll() is None:
                if os.path.exists(progress_log_file):
                    try:
                        with open(progress_log_file, 'r', encoding='utf-8') as f:
                            status = json.load(f)
                        message = f"    â””â”€â”€ çŠ¶æ€: {status['message']}"
                        if message != last_message:
                            # ä½¿ç”¨ \r å’Œ ANSI æ¸…é™¤è¡Œæ¥å®ç°åŠ¨æ€æ›´æ–°
                            print(f"\r\033[K{message}", end="", flush=True)
                            last_message = message
                    except (json.JSONDecodeError, FileNotFoundError, KeyError):
                        # æ–‡ä»¶å¯èƒ½æ­£åœ¨è¢«å†™å…¥æˆ–å·²è¢«åˆ é™¤ï¼Œå¿½ç•¥è¿™äº›ç¬æ—¶é”™è¯¯
                        pass
                time.sleep(1) # é™ä½è½®è¯¢é¢‘ç‡
            print() # è¿›åº¦æ¡ç»“æŸåæ¢è¡Œ

        # é€šä¿¡é˜¶æ®µï¼šç­‰å¾…è¿›ç¨‹ç»“æŸï¼Œç„¶åä¸€æ¬¡æ€§ã€å®‰å…¨åœ°è¯»å–æ‰€æœ‰è¾“å‡º
        stdout, stderr = process.communicate()

        # æ—¥å¿—è®°å½•é˜¶æ®µ
        if stdout:
            logger.info(f"--- å­è¿›ç¨‹ STDOUT [{description}] ---")
            for line in stdout.strip().split('\n'):
                logger.info(line)
        
        if process.returncode != 0:
            logger.error(f"å‘½ä»¤ [{description}] æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {process.returncode}ã€‚")
            if stderr:
                logger.error(f"--- å­è¿›ç¨‹ STDERR [{description}] ---")
                for line in stderr.strip().split('\n'):
                    logger.error(line)
            raise subprocess.CalledProcessError(process.returncode, command)
        
        # å¦‚æœæœ‰stderrä½†è¿”å›ç ä¸º0ï¼Œé€šå¸¸æ˜¯è­¦å‘Šä¿¡æ¯ï¼ˆä¾‹å¦‚tqdmè¿›åº¦æ¡ï¼‰
        if stderr:
            logger.debug(f"--- å­è¿›ç¨‹ STDERR (è­¦å‘Š/è°ƒè¯•ä¿¡æ¯) [{description}] ---")
            for line in stderr.strip().split('\n'):
                logger.debug(line)

        logger.info(f"âœ… å‘½ä»¤ [{description}] æ‰§è¡ŒæˆåŠŸã€‚")

    finally:
        # æ¸…ç†è¿›åº¦æ–‡ä»¶
        if use_progress_log and progress_log_file and os.path.exists(progress_log_file):
            try:
                os.remove(progress_log_file)
            except OSError:
                pass

# --- ä¸ºäº†å…¼å®¹æ€§ï¼Œä¿ç•™æ—§çš„å‡½æ•°åï¼Œä½†éƒ½æŒ‡å‘æ–°çš„ã€å¥å£®çš„å®ç° ---
def _run_command(command: str, description: str, is_gpu_intensive: bool = True):
    return _run_command_final(command, description, is_gpu_intensive, use_progress_log=False)

def _run_command_with_progress(command: str, description: str):
    return _run_command_final(command, description, is_gpu_intensive=True, use_progress_log=True)
# --- ã€æ ¸å¿ƒä¿®å¤ç»“æŸã€‘ ---


def _verify_tensor_file(path: str, expected_dims: int, num_samples: int):
    logger.info(f"æ­£åœ¨éªŒè¯å¼ é‡æ–‡ä»¶: {path}")
    if not os.path.exists(path): raise FileNotFoundError(f"è‡´å‘½é”™è¯¯ï¼šé¢„æœŸçš„è¾“å‡ºæ–‡ä»¶æœªæ‰¾åˆ°: {path}")
    tensor = torch.load(path)
    if not isinstance(tensor, torch.Tensor): raise TypeError(f"è‡´å‘½ç±»å‹é”™è¯¯ï¼šæ–‡ä»¶ {path} å†…å®¹ä¸æ˜¯ä¸€ä¸ª PyTorch å¼ é‡ã€‚")
    if tensor.dim() != expected_dims: raise ValueError(f"è‡´å‘½ç»´åº¦é”™è¯¯ï¼šæ–‡ä»¶ {path} ä¸­çš„å¼ é‡ç»´åº¦ä¸º {tensor.dim()}ï¼Œåº”ä¸º {expected_dims}ã€‚")
    if tensor.shape[0] != num_samples: raise ValueError(f"è‡´å‘½æ ·æœ¬æ•°é”™è¯¯ï¼šæ–‡ä»¶ {path} ä¸­æœ‰ {tensor.shape[0]} ä¸ªæ ·æœ¬ï¼Œä½†é¢„æœŸæœ‰ {num_samples} ä¸ªã€‚")
    if torch.isnan(tensor).any() or torch.isinf(tensor).any(): raise ValueError(f"è‡´å‘½æ•°å€¼é”™è¯¯ï¼šæ–‡ä»¶ {path} ä¸­åŒ…å« NaN æˆ– Inf å€¼ï¼")
    logger.info(f"âœ… æ–‡ä»¶ {path} éªŒè¯é€šè¿‡ã€‚å¼ é‡å½¢çŠ¶: {tensor.shape}, æ•°å€¼æœ‰æ•ˆã€‚")
    return tensor

def verify_optimizer_health(model_path: str, base_model_path: str):
    # ... (æ­¤å‡½æ•°åŠä»¥ä¸‹æ‰€æœ‰å‡½æ•°ä¿æŒä¸å˜) ...
    optimizer_file = os.path.join(model_path, "optimizer.pt")
    logger.info(f"ğŸ”¬ å¼€å§‹å¯¹ä¼˜åŒ–å™¨æ–‡ä»¶è¿›è¡Œä¸‰çº§å¥åº·æ£€æŸ¥: {optimizer_file}")
    logger.info("--- [æ£€æŸ¥ 1/3] ç»“æ„å®Œæ•´æ€§...")
    if not os.path.exists(optimizer_file): raise FileNotFoundError(f"ä¼˜åŒ–å™¨æ–‡ä»¶ä¸å­˜åœ¨: {optimizer_file}")
    try: state_dict = torch.load(optimizer_file, map_location="cpu")
    except Exception as e: raise IOError(f"ä¼˜åŒ–å™¨æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
    if "state" in state_dict: state = state_dict["state"]
    elif all(isinstance(k, int) for k in state_dict.keys()): state = state_dict
    else: raise ValueError("ä¼˜åŒ–å™¨æ–‡ä»¶ä¸­æ—¢æ²¡æœ‰ 'state' é”®ï¼Œä¹Ÿä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„çŠ¶æ€å­—å…¸ã€‚")
    if not state: raise ValueError("ä¼˜åŒ–å™¨çŠ¶æ€å­—å…¸ä¸ºç©ºï¼")
    first_state_entry = list(state.values())[0]
    if 'exp_avg' not in first_state_entry or 'exp_avg_sq' not in first_state_entry: raise ValueError("çŠ¶æ€æ¡ç›®ä¸­ç¼ºå°‘ 'exp_avg' æˆ– 'exp_avg_sq' é”®ã€‚")
    logger.info("âœ… ç»“æ„æ£€æŸ¥é€šè¿‡ã€‚")
    logger.info("--- [æ£€æŸ¥ 2/3] æ•°å€¼æœ‰æ•ˆæ€§...")
    all_m_means, all_v_means = [], []
    for i, s in enumerate(state.values()):
        m, v = s['exp_avg'], s['exp_avg_sq']
        if torch.isnan(m).any() or torch.isinf(m).any() or torch.isnan(v).any() or torch.isinf(v).any(): raise ValueError(f"ç¬¬ {i} ä¸ªå‚æ•°çš„çŠ¶æ€åŒ…å« NaN æˆ– Infï¼")
        if (v < 0).any(): raise ValueError(f"ç¬¬ {i} ä¸ªå‚æ•°çš„äºŒé˜¶çŸ©(v)åŒ…å«è´Ÿæ•°ï¼")
        all_m_means.append(m.abs().mean().item())
        all_v_means.append(v.abs().mean().item())
    avg_m_mean = sum(all_m_means)/len(all_m_means) if all_m_means else 0
    avg_v_mean = sum(all_v_means)/len(all_v_means) if all_v_means else 0
    if avg_m_mean < 1e-9 and avg_v_mean < 1e-9: raise ValueError("æ‰€æœ‰çŠ¶æ€å€¼å‡ ä¹å‡ä¸ºé›¶ï¼")
    logger.info(f"âœ… æ•°å€¼æ£€æŸ¥é€šè¿‡ã€‚å¹³å‡m: {avg_m_mean:.2e}, å¹³å‡v: {avg_v_mean:.2e}")
    logger.info("--- [æ£€æŸ¥ 3/3] ä¸æ¨¡å‹ç»“æ„åŒ¹é…...")
    try:
        model = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)
        model = PeftModel.from_pretrained(model, model_path, is_trainable=True)
        trainable_params = [p for p in model.parameters() if p.requires_grad]
        if len(state) != len(trainable_params): raise ValueError(f"ä¼˜åŒ–å™¨çŠ¶æ€æ•°é‡ ({len(state)}) ä¸æ¨¡å‹å¯è®­ç»ƒå‚æ•°æ•°é‡ ({len(trainable_params)}) ä¸åŒ¹é…ï¼")
        for i, p in enumerate(trainable_params):
            param_state = list(state.values())[i]
            if p.shape != param_state['exp_avg'].shape: raise ValueError(f"ç¬¬ {i} ä¸ªå‚æ•°çš„å½¢çŠ¶ ({p.shape}) ä¸å…¶ä¼˜åŒ–å™¨çŠ¶æ€çš„å½¢çŠ¶ ({param_state['exp_avg'].shape}) ä¸åŒ¹é…ï¼")
        logger.info("âœ… æ¨¡å‹åŒ¹é…æ£€æŸ¥é€šè¿‡ã€‚")
    except Exception as e: raise RuntimeError(f"åŠ è½½æ¨¡å‹æˆ–åŒ¹é…å‚æ•°æ—¶å‡ºé”™: {e}")
    logger.info("ğŸ‰ ä¼˜åŒ–å™¨æ–‡ä»¶å¥åº·æ£€æŸ¥å®Œæ¯•ï¼šå¥åº·ã€æœ‰æ•ˆä¸”ä¸æ¨¡å‹å®Œå…¨åŒ¹é…ï¼")

def calculate_gradients(model_path: str, data_file: str, num_samples: int, output_path: str, grad_type: str, is_lora: bool, base_model_path: str) -> str:
    if os.path.exists(output_path): shutil.rmtree(output_path)
    cmd_parts = ["python -m externals.less.data_selection.get_info", f"--model_path {model_path}", f"--base_model_path {base_model_path}", f"--train_file {data_file}", f"--output_path {output_path}", "--info_type grads", f"--gradient_type {grad_type}", f"--gradient_projection_dimension {config.GRADIENT_PROJECTION_DIM}"]
    cmd = " \\\n    ".join(cmd_parts)
    _run_command_with_progress(cmd, description=f"è®¡ç®— {os.path.basename(data_file)} çš„æ¢¯åº¦")
    expected_grad_file = os.path.join(output_path, f"dim{config.GRADIENT_PROJECTION_DIM}", "all_unnormalized.pt")
    _verify_tensor_file(expected_grad_file, expected_dims=2, num_samples=num_samples)
    return expected_grad_file

def calculate_mmlu_validation_gradients(model_path: str, output_path: str, is_lora: bool, base_model_path: str) -> str:
    if os.path.exists(output_path): shutil.rmtree(output_path)
    cmd_parts = ["python -m externals.less.data_selection.get_info", f"--model_path {model_path}", f"--base_model_path {base_model_path}", "--task mmlu", f"--data_dir {config.DATA_DIR}", f"--output_path {output_path}", "--info_type grads", f"--gradient_type {config.GRADIENT_TYPE_VALIDATION}", f"--gradient_projection_dimension {config.GRADIENT_PROJECTION_DIM}"]
    cmd = " \\\n    ".join(cmd_parts)
    _run_command_with_progress(cmd, description="è®¡ç®— MMLU éªŒè¯é›†æ¢¯åº¦")
    expected_samples = 57 * 5
    expected_grad_file = os.path.join(output_path, f"dim{config.GRADIENT_PROJECTION_DIM}", "all_unnormalized.pt")
    _verify_tensor_file(expected_grad_file, expected_dims=2, num_samples=expected_samples)
    return expected_grad_file

def run_finetuning(base_model_name: str, train_file: str, output_dir: str, num_train_samples: int) -> str:
    if os.path.exists(output_dir): shutil.rmtree(output_dir)
    sft_args = config.LORA_TRAIN_ARGS; args_list = [f"--model_name_or_path {base_model_name}", f"--train_file {train_file}", f"--output_dir {output_dir}", f"--seed {config.RANDOM_SEED}"]
    for k, v in sft_args.items():
        if isinstance(v, bool) and v: args_list.append(f"--{k}")
        elif not isinstance(v, bool): args_list.append(f"--{k} {v}")
    args_str = " ".join(args_list); cmd = f"python -m finetune {args_str}"
    _run_command(cmd, description=f"åœ¨ {num_train_samples} æ¡æ•°æ®ä¸Šå¾®è°ƒæ¨¡å‹", is_gpu_intensive=True)
    if not os.path.exists(os.path.join(output_dir, "adapter_config.json")):
        raise FileNotFoundError(f"å¾®è°ƒç»“æŸåï¼Œé¢„æœŸçš„ LoRA é…ç½®æ–‡ä»¶æœªåœ¨ {output_dir} ä¸­æ‰¾åˆ°ã€‚")
    return output_dir

def run_evaluation(model_path: str, eval_output_dir: str, eval_split: str) -> tuple[float, float]:
    split_output_dir = os.path.join(eval_output_dir, eval_split);
    if os.path.exists(split_output_dir): shutil.rmtree(split_output_dir); os.makedirs(split_output_dir)
    eval_args_str = " ".join(f"--{k} {v}" if not isinstance(v, bool) else (f"--{k}" if v else "") for k, v in config.EVAL_ARGS.items())
    split_arg = "--eval_valid" if eval_split == "dev" else ""; base_model_arg = f"--base_model_path {config.BASE_MODEL_NAME}"
    cmd = (f"python -m externals.evaluation.run_eval {eval_args_str} --model_name_or_path {model_path} --data_dir {config.MMLU_DATA_DIR} --save_dir {split_output_dir} {split_arg} {base_model_arg}")
    _run_command(cmd, description=f"åœ¨ MMLU {eval_split} é›†ä¸Šè¯„ä¼°")
    metrics_file = os.path.join(split_output_dir, "metrics.json")
    if not os.path.exists(metrics_file):
        raise FileNotFoundError(f"è¯„ä¼°ç»“æŸåï¼Œç»“æœæ–‡ä»¶ {metrics_file} æœªæ‰¾åˆ°ã€‚")
    try:
        with open(metrics_file, 'r') as f: metrics = json.load(f)
        accuracy = metrics.get("average_acc", -1.0); loss = metrics.get("average_loss", -1.0)
        logger.info(f"âœ… è¯„ä¼°æˆåŠŸã€‚Acc: {accuracy:.4f}, Loss: {loss:.4f}"); return accuracy, loss
    except (json.JSONDecodeError, KeyError) as e: raise RuntimeError(f"è§£æè¯„ä¼°ç»“æœæ–‡ä»¶ {metrics_file} æ—¶å‡ºé”™: {e}")

def run_matching_subprocess(train_grad_path: str, val_grad_path: str, selection_size: int, num_train_samples: int, phase_dir: str) -> tuple[list[int], pd.DataFrame]:
    output_dir = os.path.join(phase_dir, "matching_results")
    os.makedirs(output_dir, exist_ok=True)
    output_indices_path = os.path.join(output_dir, "top_indices.pt")
    output_scores_path = os.path.join(output_dir, "all_scores.jsonl")
    cmd_parts = ["python", "run_matching.py", f"--train_grad_path {train_grad_path}", f"--val_grad_path {val_grad_path}", f"--num_train_samples {num_train_samples}", f"--selection_size {selection_size}", f"--output_indices_path {output_indices_path}", f"--output_scores_path {output_scores_path}"]
    cmd = " ".join(cmd_parts)
    _run_command(cmd, description="æ‰§è¡Œåˆ†æ•°è®¡ç®—å­è¿›ç¨‹")
    if not os.path.exists(output_indices_path) or not os.path.exists(output_scores_path):
        raise RuntimeError("åˆ†æ•°è®¡ç®—å­è¿›ç¨‹æœªèƒ½æˆåŠŸç”Ÿæˆè¾“å‡ºæ–‡ä»¶ï¼")
    top_k_indices = torch.load(output_indices_path).tolist()
    scores_df = pd.read_json(output_scores_path, lines=True)
    logger.info(f"âœ… æˆåŠŸä»å­è¿›ç¨‹åŠ è½½åˆ†æ•°å’Œç´¢å¼•ã€‚é€‰æ‹©äº† {len(top_k_indices)} ä¸ªç´¢å¼•ã€‚")
    return top_k_indices, scores_df